<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Layers · Photon</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../"><img class="logo" src="../assets/logo.png" alt="Photon logo"/></a><h1>Photon</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><span class="toctext">API</span><ul><li><a class="toctext" href="../core/">Core</a></li><li class="current"><a class="toctext" href>Layers</a><ul class="internal"><li><a class="toctext" href="#Basic-1">Basic</a></li><li><a class="toctext" href="#Container-1">Container</a></li><li><a class="toctext" href="#Convolutional-1">Convolutional</a></li><li><a class="toctext" href="#Pooling-1">Pooling</a></li><li><a class="toctext" href="#Recurrent-1">Recurrent</a></li></ul></li><li><a class="toctext" href="../losses/">Losses</a></li><li><a class="toctext" href="../metrics/">Metrics</a></li><li><a class="toctext" href="../data/">Data</a></li></ul></li><li><a class="toctext" href="../community/">Community</a></li></ul></nav><article id="docs"><header><nav><ul><li>API</li><li><a href>Layers</a></li></ul><a class="edit-page" href="https://github.com/neurallayer/Photon.jl/blob/master/docs/src/layers.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Layers</span><a class="fa fa-bars" href="#"></a></div></header><h2><a class="nav-anchor" id="Basic-1" href="#Basic-1">Basic</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.Dense" href="#Photon.Dense"><code>Photon.Dense</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Regular densely-connected NN layer.</p><p>Dense implements the operation: output = activation(dot(input, weight) + bias) where activation is the element-wise activation function passed as the activation argument, weight is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is true).</p><p><strong>Usage</strong></p><pre><code class="language-julia">layer = Dense(10, relu)
layer = Dense(100, use_bias=false)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/core.jl#L52-L67">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.Dropout" href="#Photon.Dropout"><code>Photon.Dropout</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Dropout layer with optional the rate (between 0 and 1) of dropout. If no rate is specified, 0.5 (so 50%) will be used.</p></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/core.jl#L136-L139">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.BatchNorm" href="#Photon.BatchNorm"><code>Photon.BatchNorm</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Batch normalization layer (Ioffe and Szegedy, 2014). Normalizes the input at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.</p><p>Finally, if activation is not nothing, it is applied to the outputs as well.</p></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/core.jl#L154-L160">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.Flatten" href="#Photon.Flatten"><code>Photon.Flatten</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Flattening Layer. Photon by default already has flattening funcitonality build into the Dense layer, so you won&#39;t need to include a separate Flatten layer before a Dense layer.</p></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/core.jl#L111-L115">source</a></section><h2><a class="nav-anchor" id="Container-1" href="#Container-1">Container</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.Sequential" href="#Photon.Sequential"><code>Photon.Sequential</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Sequential layer allows to chain together a number of other layers.</p><p><strong>Usage</strong></p><pre><code class="language-julia">model = Sequential(Conv2D(100),MaxPool(),Dense(10))</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/container.jl#L16-L25">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.Concurrent" href="#Photon.Concurrent"><code>Photon.Concurrent</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Concurrrent layer allows for stacking a number of other layers in parallel and combining their results before returning it.</p><p>This layer will stack on the second last dimension. So with 2D and 3D convolution this will be the channel layer (WxHxCxN). As a result other dimensions have to the same.</p></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/container.jl#L40-L47">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.Residual" href="#Photon.Residual"><code>Photon.Residual</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Residual Layer works like a Sequential layer, however before returning the result it will be combined with the orginal input (residual). This is a popular techique in modern neural networds since it allows for better backpropagation.</p><p>This will stack on the second last dimension. So with 2D and 3D convolution this will be the channel layer (WxHxCxN)</p></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/container.jl#L63-L70">source</a></section><h2><a class="nav-anchor" id="Convolutional-1" href="#Convolutional-1">Convolutional</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.Conv2D" href="#Photon.Conv2D"><code>Photon.Conv2D</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>2D convolution layer (e.g. spatial convolution over images).</p><p>This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is true, a bias vector is created and added to the outputs. Finally, if activation is not nothing, it is applied to the outputs as well.</p></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/conv.jl#L108-L115">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.Conv2DTranspose" href="#Photon.Conv2DTranspose"><code>Photon.Conv2DTranspose</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>ConvTranspose layer</p></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/conv.jl#L129-L131">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.Conv3D" href="#Photon.Conv3D"><code>Photon.Conv3D</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>3D convolution layer (e.g. spatial convolution over volumes).</p><p>This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is true, a bias vector is created and added to the outputs. Finally, if activation is not nothing, it is applied to the outputs as well.</p></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/conv.jl#L119-L126">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.Conv3DTranspose" href="#Photon.Conv3DTranspose"><code>Photon.Conv3DTranspose</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>ConvTranspose layer</p></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/conv.jl#L129-L131">source</a></section><h2><a class="nav-anchor" id="Pooling-1" href="#Pooling-1">Pooling</a></h2><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MaxPool2D</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>AvgPool2D</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>MaxPool3D</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition warning"><div class="admonition-title">Missing docstring.</div><div class="admonition-text"><p>Missing docstring for <code>AvgPool3D</code>. Check Documenter&#39;s build log for details.</p></div></div><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.AdaptiveAvgPool" href="#Photon.AdaptiveAvgPool"><code>Photon.AdaptiveAvgPool</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Adaptive Average Pool has a fixed size output and enables creating a convolutional network that can be used for multiple image formats.</p></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/conv.jl#L233-L236">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.AdaptiveMaxPool" href="#Photon.AdaptiveMaxPool"><code>Photon.AdaptiveMaxPool</code></a> — <span class="docstring-category">Type</span>.</div><div><div><p>Adaptive MaxPool has a fixed size output and enables creating a convolutional network that can be used for multiple image formats.</p></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/conv.jl#L279-L282">source</a></section><h2><a class="nav-anchor" id="Recurrent-1" href="#Recurrent-1">Recurrent</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.RNN" href="#Photon.RNN"><code>Photon.RNN</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>A simple RNN layer.</p></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/recurrent.jl#L67-L69">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.LSTM" href="#Photon.LSTM"><code>Photon.LSTM</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</p><p>For each element in the input sequence, each layer computes the following function:</p><p><span>$\begin{array}{ll} i_t = sigmoid(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\f_t = sigmoid(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\o_t = sigmoid(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\c_t = f_t * c_{(t-1)} + i_t * g_t \\h_t = o_t * \tanh(c_t) \end{array}$</span></p><p><strong>Examples:</strong></p><pre><code class="language-julia">layer = LSTM(50)
layer = LSTM(50, 2, bidirectional=true)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/recurrent.jl#L87-L109">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Photon.GRU" href="#Photon.GRU"><code>Photon.GRU</code></a> — <span class="docstring-category">Function</span>.</div><div><div><p>Create a GRU layer</p><p><strong>Examples:</strong></p><pre><code class="language-none">layer = GRU(50)</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/neurallayer/Photon.jl/blob/c0e6e40ef4173c6f5475f030b68f3b0fb78aa6c1/src/layers/recurrent.jl#L125-L133">source</a></section><footer><hr/><a class="previous" href="../core/"><span class="direction">Previous</span><span class="title">Core</span></a><a class="next" href="../losses/"><span class="direction">Next</span><span class="title">Losses</span></a></footer></article></body></html>
