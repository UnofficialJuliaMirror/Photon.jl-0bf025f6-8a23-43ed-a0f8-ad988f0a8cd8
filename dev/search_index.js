var documenterSearchIndex = {"docs":
[{"location":"api/#API-1","page":"API","title":"API","text":"","category":"section"},{"location":"api/#","page":"API","title":"API","text":"Modules = [Photon]\nOrder   = [:function, :type]","category":"page"},{"location":"api/#Photon.GRU","page":"API","title":"Photon.GRU","text":"Create a GRU layer\n\nExamples:\n\nlayer = GRU(50)\n\n\n\n\n\n","category":"function"},{"location":"api/#Photon.KorA-Tuple{Array}","page":"API","title":"Photon.KorA","text":"KorA makes it easy to move an array to the GPU or the other way around\n\n\n\n\n\n","category":"method"},{"location":"api/#Photon.LSTM","page":"API","title":"Photon.LSTM","text":"Create a LSTM layer.\n\nExamples:\n\nlayer = LSTM(50)\n\n\n\n\n\n","category":"function"},{"location":"api/#Photon.RNN","page":"API","title":"Photon.RNN","text":"A simple RNN layer.\n\n\n\n\n\n","category":"function"},{"location":"api/#Photon.fit!","page":"API","title":"Photon.fit!","text":"Train the model based on a supervised dataset and for a number of epochs. fit! can be called multiple times and will continue to train where is left of last time.\n\nBy default the fit! function will try to ensure the data is of the right type (e.g. Float32) and on the right device (e.g. GPU) before feeding it to the model.\n\nUsage\n\nfit!(workout, traindata)\nfit!(workout, traindata, testdata, epochs=50)\n\nIf you don't want any data conversion, just pass the identity funciton as the convertor:\n\nfit!(workout, traindata, convertor=identity)\n\n\n\n\n\n","category":"function"},{"location":"api/#Photon.freeze!-Tuple{AutoGrad.Param}","page":"API","title":"Photon.freeze!","text":"Freeze a parameter so it no longer will be updated during training.\n\n\n\n\n\n","category":"method"},{"location":"api/#Photon.hasmetric-Tuple{Workout,Symbol}","page":"API","title":"Photon.hasmetric","text":"Does the workout have any recorded values for a certain metric\n\n\n\n\n\n","category":"method"},{"location":"api/#Photon.loadWorkout-Tuple{Any}","page":"API","title":"Photon.loadWorkout","text":"Load a workout from file and return it.\n\nUsage\n\nworkout = loadWorkout(\"workout_1000.dat\")\nfit!(workout, mydata)\n\n\n\n\n\n","category":"method"},{"location":"api/#Photon.output_size-Tuple{Photon.Conv,Any}","page":"API","title":"Photon.output_size","text":"Utility function to determine the output size of a convolutional layer given a certain input size and configuration of a convolutional layer. This function works even if the weights of the layer are not initialized.\n\nFor example: \tc = Conv2D(16, (3,3), strides=3, padding=1) \toutput_size(c, (224,224)) # output is (75, 75)\n\n\n\n\n\n","category":"method"},{"location":"api/#Photon.plotmetrics","page":"API","title":"Photon.plotmetrics","text":"Plot the metrics after some training. This function will plot all the metrics in a single graph.\n\nIn order to avoid Photon being dependend on Plots, the calling code will have to provide that module as the first parameter.\n\nUsage\n\nfit!(workout, mydata, epochs=10)\n\nimport Plots\nplotmetrics(Plots, workout)\n\n\n\n\n\n","category":"function"},{"location":"api/#Photon.predict-Tuple{Any,Any}","page":"API","title":"Photon.predict","text":"Predict a sample, either a single value or a batch. Compared to invoking the model directory with model(x), predit takes care of:\n\nMoving the data to the GPU if required.\nMaking the data into a batch (controlled by makebatch parameter)\n\nUsage\n\nx = randn(Float32, 224, 224, 3)\npredict(model, x)\n\n\n\n\n\n","category":"method"},{"location":"api/#Photon.saveWorkout","page":"API","title":"Photon.saveWorkout","text":"Save a workout to a file. This will save all the state that is captured in the workout and enables to continue at a later stage.\n\n\n\n\n\n","category":"function"},{"location":"api/#Photon.unfreeze!-Tuple{AutoGrad.Param}","page":"API","title":"Photon.unfreeze!","text":"Unfreeze a parameter so it will be updated again during training.\n\n\n\n\n\n","category":"method"},{"location":"api/#Photon.AdaptiveAvgPool","page":"API","title":"Photon.AdaptiveAvgPool","text":"Adaptive Average Pool has a fixed size output and enables creating a convolutional network that can be used for multiple image formats.\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.AdaptiveMaxPool","page":"API","title":"Photon.AdaptiveMaxPool","text":"Adaptive MaxPool has a fixed size output and enables creating a convolutional network that can be used for multiple image formats.\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.BCELoss","page":"API","title":"Photon.BCELoss","text":"Binary CrossEntropy\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.BatchNorm","page":"API","title":"Photon.BatchNorm","text":"BatchNorm layer with support for an optional activation function\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.Concurrent","page":"API","title":"Photon.Concurrent","text":"Concurrrent layer allows for stacking a number of other layers in parallel and combining their results before returning it.\n\nThis layer will stack on the second last dimension. So with 2D and 3D convolution this will be the channel layer (WxHxCxN). As a result other dimensions have to the same.\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.ContextSwitch","page":"API","title":"Photon.ContextSwitch","text":"Beginning of allowing for a single model instance to run on multiple devices (expiremental)\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.CrossEntropyLoss","page":"API","title":"Photon.CrossEntropyLoss","text":"CrossEntropy loss function with support for an optional weight parameter. The weight parameter can be static (for example to handle class inbalances) or dynamic (so passed every time when the lost function is invoked)\n\nUsage\n\nworkout = Workout(model, CE(), SGD())\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.Dense","page":"API","title":"Photon.Dense","text":"Fully connected layer with an optional bias weight.\n\nUsage\n\nlayer = Dense(10, relu)\nlayer = Dense(100, use_bias=false)\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.Dropout","page":"API","title":"Photon.Dropout","text":"Dropout layer with optional the rate (between 0 and 1) of dropout. If no rate is specified, 0.5 (so 50%) will be used.\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.Flatten","page":"API","title":"Photon.Flatten","text":"Flattening Layer. Photon by default already has flattening funcitonality build into the Dense layer, so you won't need to include a separate Flatten layer before a Dense layer.\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.HingeLoss","page":"API","title":"Photon.HingeLoss","text":"Hinge Loss implementation\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.L1Loss","page":"API","title":"Photon.L1Loss","text":"L1Loss also known as MAE loss\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.L2Loss","page":"API","title":"Photon.L2Loss","text":"L2Loss also known as MSE loss\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.LNLoss","page":"API","title":"Photon.LNLoss","text":"LNLoss\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.PseudoHuberLoss","page":"API","title":"Photon.PseudoHuberLoss","text":"Pseudo Huber Loss implementation, somewhere between a L1 and L2 loss.\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.Residual","page":"API","title":"Photon.Residual","text":"Residual Layer works like a Sequential layer, however before returning the result it will be combined with the orginal input (residual). This is a popular techique in modern neural networds since it allows for better backpropagation.\n\nThis will stack on the second last dimension. So with 2D and 3D convolution this will be the channel layer (WxHxCxN)\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.Sequential","page":"API","title":"Photon.Sequential","text":"Sequential layer allows to chain together a number of other layers.\n\nUsage\n\nmodel = Sequential(Conv2D(100),MaxPool(),Dense(10))\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.Workout","page":"API","title":"Photon.Workout","text":"The Workout keeps track of the progress of the training session. At least a model and a loss function needs to be provided. Optional an optimizer and one or more metrics can be specified.\n\nIf no optimizer is provided, SGD will be used. If no metrics are provided, only the loss during training and validation will be registered (:loss and :val_loss).\n\nUsage\n\nworkout = Workout(model, mse)\n\nworkout = Workout(model, nll, SGD())\n\nworkout = Workout(model, nll, SGD(); acc=BinaryAccuracy())\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.autoConvertor-Tuple{Array}","page":"API","title":"Photon.autoConvertor","text":"autoConvertor converts data to the right format for a model. It uses the context to determine the device (cpu or gpu) and datatype that the data needs to be.\n\nIt supports Tuples, Arrays and KnetArrays and a combination of those.\n\n\n\n\n\n","category":"method"},{"location":"api/#Photon.back!-Tuple{AutoGrad.Tape,Any}","page":"API","title":"Photon.back!","text":"Perform the back propagation and update of weights in one go.\n\n\n\n\n\n","category":"method"},{"location":"api/#Photon.getmetricname","page":"API","title":"Photon.getmetricname","text":"Function to generate the fully qualified metric name. It uses the metric name and phase (train or valid) to come up with a unique name.\n\n\n\n\n\n","category":"function"},{"location":"api/#Photon.getmetricvalue","page":"API","title":"Photon.getmetricvalue","text":"Get the metric value for a fully qualified metric name and a certain step. If step is not provided the last step will be used. If no value is found the passed function will not be invoked.\n\nUsage\n\ngetmetricvalue(workout, :val_loss) do value\n    println(\"validation loss\", value)\nend\n\n\n\n\n\n","category":"function"},{"location":"api/#Photon.gradients","page":"API","title":"Photon.gradients","text":"Utility function to calculate the gradients. Useful when checking for vanishing or exploding gradients. The returned value is a Vector of (Param, Gradient).\n\n\n\n\n\n","category":"function"},{"location":"api/#Photon.step!-Tuple{Workout,Any,Any}","page":"API","title":"Photon.step!","text":"Take a single step in updating the weights of a model. This function will be invoked from fit! to do the actual learning.\n\nFor a minibatch (x,y) of data, the folowing sequence will be executed:\n\nperform the forward pass\ncalculate the loss\nupdate and remember the metrics, if any\ndo the backpropagation and update the weights\n\n\n\n\n\n","category":"method"},{"location":"api/#Photon.stop-Tuple{Workout,String}","page":"API","title":"Photon.stop","text":"Stop a training session. If this is called outside the scope of a trianing session, just an error is thrown.\n\n\n\n\n\n","category":"method"},{"location":"api/#Photon.updatemetrics!","page":"API","title":"Photon.updatemetrics!","text":"Invoke the configured metrics. The loss metric will always be logged and available. Metrics are stored in the history attribute of the workout.\n\n\n\n\n\n","category":"function"},{"location":"api/#Photon.validate-Tuple{Workout,Any,Any}","page":"API","title":"Photon.validate","text":"Validate a minibatch and calculate the loss and metrics. Typically this function is called from the fit! method.\n\n\n\n\n\n","category":"method"},{"location":"api/#Serialization.serialize-Tuple{Serialization.AbstractSerializer,Knet.KnetArray}","page":"API","title":"Serialization.serialize","text":"Enable saving and loading of models by specialized KnetArray methods for Julia serialization This will effectively move a GPU weight to the CPU before serialing it and move it back to the GPU when deserializing.\n\n\n\n\n\n","category":"method"},{"location":"api/#Photon.Context","page":"API","title":"Photon.Context","text":"Context is used by various parts of Photon to determine what the device and datatype should be for Arrays. It also allows to quickly switch between GPU and CPU based models.\n\nAttributes\n\ndevice::Symbol the type of device. For now supported :cpu and :gpu\ndeviceId::Int the id of the device, useful for example if you multiple GPU's\ndtype::Type the type of data you want to use. Most common are Float32, Float16 or Float64.\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.Conv","page":"API","title":"Photon.Conv","text":"Convolutional layer that serves as the base for Conv2D and Conv3D\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.ConvTranspose","page":"API","title":"Photon.ConvTranspose","text":"ConvTranspose layer\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.Loss","page":"API","title":"Photon.Loss","text":"Base type for the loss functions. However Photon accepts any functoon as a loss function as long as it is callable and returns the loss as a scalar value.\n\n fn(y_pred, y_true) :: Number\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.Meter","page":"API","title":"Photon.Meter","text":"A meter is reponsible for presenting metric values. This can be printing it to the console output, showing it on TensorBoard of storing it in a database.\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.PoolingLayer","page":"API","title":"Photon.PoolingLayer","text":"Pooling layers\n\n\n\n\n\n","category":"type"},{"location":"api/#Photon.StackedLayer","page":"API","title":"Photon.StackedLayer","text":"Common behavior for stacked layers that enables to access them as arrays\n\n\n\n\n\n","category":"type"},{"location":"community/#Community-1","page":"Community","title":"Community","text":"","category":"section"},{"location":"community/#","page":"Community","title":"Community","text":"All Photon users are welcome to ask questions on the Julia forum. Of course issues can be opened on Github where you also can get the source code.","category":"page"},{"location":"#Introduction-1","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Photon is a developer friendly framework for Machine Learning in Julia. Under the hood it leverages Knet and it provides a Keras like API on top of that.","category":"page"},{"location":"#Steps-1","page":"Home","title":"Steps","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"To train your own model, there are four steps to follow:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Create your model using the layers that come out of the box with Photon or using your own custom layers.\nCreate a workout that combines the model, a loss function and an optimiser. Optionally you can also add some metrics that you want to monitor.\nPrepare your data with a Data pipeline\nTrain the model by calling fit! on the workout and the training data.","category":"page"},{"location":"#Step-1:-Create-a-Model-1","page":"Home","title":"Step 1: Create a Model","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"A model can use out of the box layers or your own layers. Photon support most common type of layer:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Dense or fully connected layers\n2D ad 3D convolutional layers\nDifferent type of Pooling layers\nRecurrent layers (RNN, LSTM, GRU)\nDropout layers\nSeveral types of container layers like Sequential and Residual","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Some examples how to create the different type of models:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"mymodel = Sequential(\n            Dense(64, relu),\n            Dense(10)\n)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"mymodel = Sequential(\n            Conv2D(64, 3, relu),\n            Conv2D(64, 3, relu),\n            MaxPool(),\n            Dense(10)\n)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"mymodel = Sequential(\n            LSTM(64),\n            Dense(64),\n            Dense(10)\n)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"So normally you won't need to create your own layers. But if you have to, a layer is nothing more than function. So the following could be a layer ;)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"myLayer(X) = moon == :full ? X .- 1 : X","category":"page"},{"location":"#Step-2:-Define-a-Workout-1","page":"Home","title":"Step 2: Define a Workout","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"A workout combines a model + loss + optimiser and keeps track of the progress during the actual training. The workout is stateful in the sense that you can run multiple training sessions and the progress will be recorded appropriately.   ","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The minimum required code to create a workout is:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"workout = Workout(mymodel, MSELoss())","category":"page"},{"location":"#","page":"Home","title":"Home","text":"This will create a workout that will use the default Optimiser (SGD) and only the loss metric being tracked.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Alternatively you can pass an optimizer and define the metrics that you want to get tracked during the training sessions. Photon tracks :loss and :val_loss (for the validation phase) by default, but you define additional ones.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"workout = Workout(mymodel, MSE(), SGD(), acc=accuracy())","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Another useful feature is that a Workout can saved and restored at any moment during the training. And not only the model and its parameters will be saved. Also the state of the optimiser and any defined metrics will be able saved and restored to their previous state. This even makes it also possible to shared workout with colleagues (although they need the same packages installed installed as you).","category":"page"},{"location":"#","page":"Home","title":"Home","text":"filename = saveWorkout(workout)\n\nworkout2 = loadWorkout(filename)","category":"page"},{"location":"#Step-3:-Prepare-the-Data-1","page":"Home","title":"Step 3: Prepare the Data","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Although Photon is perfectly happy to work on plain Vectors of data, this often won't be feasible due to the data not fitting in memory. In those cases you can use the data pipeline feature of Photon.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"A typical pipeline would look something like this:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"  data = SomeDataset(source) |> SomeTransformers() |> MiniBatch()","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Add then the pipeline can be used directly in the training cycle:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"  fit!(workout, data)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Photon comes out the box with several reusable components for creating these pipelines. They can be divided into two types; Datasets that are the start of a pipeline and retrieve the data from some source and Transformers that transform the output of a previous step.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Source datasets","category":"page"},{"location":"#","page":"Home","title":"Home","text":"ImageDataset\nTestDataset\nArrayDataset\nJLDDataset","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Transformers","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Normalizer\nCropper\nMiniBatch\nNoiser","category":"page"},{"location":"#","page":"Home","title":"Home","text":"A complete pipeline for image data could look something like this:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"data = ImageDataset(files, labels, resize=(250,250))\ndata = data |> Crop(200,200) |> Normalize() |> MiniBatch(8)","category":"page"},{"location":"#Step-4:-Run-the-Training-1","page":"Home","title":"Step 4: Run the Training","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"The actual training in Photon is done invoking the fit! function.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"fit!(workout, data, epochs=5)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The validation phase is optional. But if you provide data for the validation phase, Photon will automatically run a validation after each training epoch.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"fit!(workout, data, training_data, epochs=10)","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Defined metrics and loss will then be available both for training and validation.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"The data is expected to be a tuple of (X, Y) where X and Y can be tuples again in case your model expects multiple inputs or outputs. So some examples of valid formats","category":"page"},{"location":"#","page":"Home","title":"Home","text":"(X,Y)\n((X1,X2), Y)\n(X, (Y1, Y2, Y3))\n((X1, X2), (Y1, Y2))","category":"page"},{"location":"#","page":"Home","title":"Home","text":"By default fit! will convert each batch to the right data type and device. This is controlled by the optional parameter convertor. If you don't want a conversion to take place and ensured the provided data is already in the right format, you can pass the identity function:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"fit!(workout, data; convertor=identity)","category":"page"}]
}
